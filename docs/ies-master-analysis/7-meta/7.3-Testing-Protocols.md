# 7.3 Testing Protocols

**Purpose:** Testing methodologies for validating IES capabilities against core principles
**Last Updated:** December 6, 2025
**Philosophy:** Test principles, not just code

---

## Testing Philosophy

### What We Test

IES testing goes beyond code coverage to validate:

1. **Principle Adherence** — Does it deliver on core promises?
2. **Thinking Partnership** — Does AI actually guide cognition?
3. **ADHD-Friendly UX** — Is friction truly low?
4. **Virtuous Cycle** — Do loops close and enrich each other?
5. **User Experience** — Can users achieve their goals?

### Three Testing Layers

```
┌─────────────────────────────────────────────────┐
│  USER TESTING (Real Usage)                      │
│  Does Chris use it daily? Does it help?         │
└─────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────┐
│  INTEGRATION TESTING (Cross-Component)          │
│  Do layers work together? Data flows complete?  │
└─────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────┐
│  UNIT TESTING (Component Isolation)             │
│  Do APIs return correct data? Schemas valid?    │
└─────────────────────────────────────────────────┘
```

**Current Status:**
- ✅ Unit: 94/94 backend tests passing (100%)
- ⚠️ Integration: Manual testing only
- ⚠️ User: Validated in Phase 1, needs continuous validation

---

## Capability Testing Protocols

### Capture Mode Testing

**Goal:** Validate zero-friction thought capture

#### Unit Tests

**Location:** `ies/backend/tests/test_capture.py`

**Test Cases:**
- ✅ Create capture item with entities
- ✅ List captures filtered by status
- ✅ Update capture metadata
- ✅ Process capture (status transition)

**Coverage:** 4/4 tests passing

#### Integration Tests

**Scenario 1: Quick Capture → Classified**
```
1. User opens Quick Capture in SiYuan
2. Types raw thought: "Shame might block executive function access"
3. Clicks "Capture"
4. Expected:
   - CaptureItem created in backend with status='raw'
   - AI extracts entities: ["Shame", "Executive Function"]
   - Status updates to 'classified'
   - Auto_summary generated
   - SiYuan block created in /Daily/
5. Verification:
   - Check Neo4j for CaptureItem node
   - Verify status='classified'
   - Confirm entities extracted
   - SiYuan block has custom-be_id attribute
```

**Scenario 2: Capture from Readest Highlight**
```
1. User highlights text in Readest: "The narrow window of awareness..."
2. Clicks "Capture"
3. Expected:
   - CaptureItem with capture_channel='readest'
   - Book metadata attached (calibre_id, title, page)
   - Context snippet stored
   - Entities extracted from highlight
4. Verification:
   - CaptureItem has book_title, book_author fields
   - Backend linked via be_id
   - Appears in SiYuan Quick Capture queue
```

**Performance Target:**
- Capture latency: <500ms from click to confirmation
- AI classification: <2s for typical capture

#### User Testing Protocol

**Week 1 Validation:**
- [ ] Capture 20+ sparks over 7 days
- [ ] Measure: Did any captures NOT happen due to friction?
- [ ] Measure: How many captures processed within 24 hours?
- [ ] Feedback: Does capture feel frictionless?

**Success Criteria:**
- ✅ Capture completion rate >95% (if thought occurred, it got captured)
- ✅ Processing within 24 hours for 80% of captures
- ✅ User reports "no friction" or "minimal friction"

---

### Dialogue Mode Testing

**Goal:** Validate AI-guided meaning extraction through personalized questions

#### Unit Tests

**Location:** `ies/backend/tests/test_question_engine.py`

**Test Cases:**
- ✅ State detection (50 tests)
- ✅ Question classification (9 classes)
- ✅ Approach selection based on state
- ✅ Template rendering with context
- ✅ Profile adaptation

**Coverage:** 50/50 tests passing

#### Integration Tests

**Scenario 1: ForgeMode Learning Session**
```
1. User opens ForgeMode in SiYuan
2. Selects "Learning" mode
3. Topic: "How does shame block metabolization?"
4. Expected:
   - Backend /templates/learning-mechanism-map returns template
   - AI generates Schema-Probe question first
   - User responds → question-response tracked
   - AI asks Causal question next
   - Session document created in /Sessions/Learning/
5. Verification:
   - ThinkingSession created in Neo4j
   - Question classes used: [Schema-Probe, Causal, ...]
   - Session document has frontmatter with be_id
   - Transcript includes all Q&A exchanges
```

**Scenario 2: Question Class Cognitive Hints**
```
1. User sees question in ForgeMode
2. Question class badge visible (e.g., "Schema-Probe")
3. Click badge to expand hint
4. Expected:
   - Hint text specific to question class
   - Sentence starters provided (optional)
   - User types response using hint guidance
5. Verification:
   - Hint matches QUESTION_CLASS_HINTS in ForgeMode.svelte
   - Response captured in questionResponseHistory
   - Journey breadcrumb includes question exchange
```

**Performance Target:**
- Question generation: <3s per question
- Response processing: <2s
- Session save: <1s

#### User Testing Protocol

**Session Quality Assessment:**
- [ ] Run 10 ForgeMode sessions (2 per mode)
- [ ] Rate each session: "Did questions guide my thinking?" (1-5)
- [ ] Count: How many questions felt generic vs. personalized?
- [ ] Measure: Did session surface insights I couldn't reach alone?

**Success Criteria:**
- ✅ Average rating ≥4.0 for question quality
- ✅ <20% questions feel generic
- ✅ 80% of sessions produce at least one novel insight

---

### Flow Mode Testing

**Goal:** Validate non-linear graph exploration with thinking partner

#### Unit Tests

**Location:** `ies/backend/tests/test_thinking_and_flow.py`

**Test Cases:**
- ✅ Open flow session from thinking session
- ✅ Record breadcrumb steps
- ✅ Track visited nodes and edges
- ✅ Generate journey synthesis
- ✅ Recommend next paths

**Coverage:** 5/5 tests passing

#### Integration Tests

**Scenario 1: Entity Click → Flow Panel**
```
1. User reading book in Readest with entity overlay ON
2. Click highlighted entity: "Executive Function"
3. Expected:
   - Flow Panel opens (or updates if already open)
   - Entity definition displayed
   - Relationships grouped by type (supports, component_of, etc.)
   - Sources listed (books mentioning this entity)
   - Thinking partner questions generated
   - Journey step added to breadcrumb
4. Verification:
   - FlowSession created/updated in Neo4j
   - Breadcrumb shows entity name + dwell time
   - Questions specific to entity context
```

**Scenario 2: Flow Exploration Journey**
```
1. User starts at "Shame" entity
2. Clicks relationship: "blocks" → "Metabolization"
3. Clicks relationship: "requires" → "Nervous System Configuration"
4. Bookmarks question: "How does shame prevent NS regulation?"
5. Expected:
   - Journey breadcrumb shows 3 steps
   - Dwell time tracked per entity
   - Bookmark saved as journey mark
   - Synthesis generated from path
6. Verification:
   - FlowSession.breadcrumbs has 3 entries
   - Journey marks counter shows 1
   - visited_nodes = ["Shame", "Metabolization", "Nervous System Configuration"]
   - visited_edges tracked
```

**Scenario 3: Cross-App Journey Resume**
```
1. User explores in Readest (3 entities visited)
2. Closes Readest, opens SiYuan
3. Opens Flow Mode in SiYuan
4. Expected:
   - Recent journey visible in suggestions
   - Click "Resume Journey" → loads last 3 entities
   - Journey breadcrumb continues from where left off
5. Verification:
   - Journey retrieved from backend by user_id
   - Breadcrumb state synced
   - Next steps continue same flow_session_id
```

**Performance Target:**
- Entity lookup: <200ms
- Relationship query: <500ms
- Journey save: <300ms
- Synthesis generation: <5s

#### User Testing Protocol

**Exploration Quality Assessment:**
- [ ] Complete 5 explorations (each 10+ steps)
- [ ] Rate: "Did exploration reveal unexpected connections?" (1-5)
- [ ] Count: Novel insights per exploration
- [ ] Measure: Do thinking partner questions help navigation?

**Success Criteria:**
- ✅ Average rating ≥4.0 for connection revelation
- ✅ At least 1 novel insight per 10-step exploration
- ✅ 70% of questions rated helpful for navigation

---

## Cross-Component Integration Tests

### Virtuous Cycle Validation

**Goal:** Verify that loops close and enrich each other

#### Test: Capture → Dialogue → Graph → Flow

```
End-to-End Flow:
1. CAPTURE: User captures thought in SiYuan Quick Capture
   → CaptureItem created with status='raw'

2. CLASSIFY: AI processes capture
   → Entities extracted, status='classified'

3. DIALOGUE: User processes capture in ForgeMode
   → ThinkingSession created, angles explored
   → Session document saved with be_id

4. GRAPH: Session entities materialized
   → Neo4j nodes created for extracted concepts
   → Relationships inferred from dialogue

5. FLOW: User explores graph from new concept
   → FlowSession opened
   → Journey captured, breadcrumbs tracked

6. ENRICHMENT: Journey insights fed back
   → New sparks captured during flow
   → Cycle restarts at deeper level

Verification:
- Trace single concept from capture → graph
- Confirm journey includes original concept
- New sparks reference original concept
- Graph has bidirectional links
```

#### Test: Profile → Questions → Journey → Profile

```
Personalization Loop:
1. BASELINE: User profile initialized (6 dimensions)
   → structure_preference, pace_preference, etc.

2. DIALOGUE: Questions adapt to profile
   → High structure → more Schema-Probe questions
   → Low ambiguity tolerance → more Anchor questions

3. JOURNEY: Exploration patterns observed
   → User always explores causal relationships
   → Preference for concrete examples

4. LEARNING: Profile updates from behavior
   → verification_approach refined
   → novelty_response adjusted

5. ADAPTATION: Next questions reflect learning
   → More Causal questions offered
   → Anchor questions prioritized

Verification:
- Profile dimensions change over 10 sessions
- Question distribution shifts with profile
- User reports questions "feel more natural"
```

---

## Performance Testing

### Load Testing Scenarios

#### Scenario 1: Large Graph Performance

**Setup:**
- Neo4j with 1000+ entities
- 2000+ relationships
- 100+ books indexed

**Tests:**
```python
# Entity search performance
def test_entity_search_1000_nodes():
    start = time.time()
    results = graph_service.search_entities(query="executive function")
    elapsed = time.time() - start
    assert elapsed < 0.5  # <500ms
    assert len(results) > 0

# Relationship traversal performance
def test_relationship_query_depth_3():
    start = time.time()
    neighbors = graph_service.explore_entity(entity_id, depth=3)
    elapsed = time.time() - start
    assert elapsed < 1.0  # <1s for 3-hop neighborhood
    assert len(neighbors) > 0
```

**Performance Targets:**
- Entity search: <500ms for any query
- Relationship query (depth 3): <1s
- Journey synthesis: <5s

#### Scenario 2: Entity Overlay with 200+ Entities

**Setup:**
- Book with 200+ mentioned entities
- Entity overlay ON in Readest
- All entity types enabled

**Tests:**
```javascript
// Entity highlight performance
test('Entity overlay renders in <2s', async () => {
  const start = Date.now();
  const entities = await fetchEntitiesByBook(calibreId);
  const transformed = transformContent(content, entities);
  const elapsed = Date.now() - start;
  expect(elapsed).toBeLessThan(2000);
  expect(entities.length).toBeGreaterThan(200);
});

// Trie-based matching prevents catastrophic backtracking
test('No performance degradation with 500 entities', async () => {
  const entities = generateEntities(500);
  const start = Date.now();
  const result = processTextWithTrie(longText, entities);
  const elapsed = Date.now() - start;
  expect(elapsed).toBeLessThan(3000);  // Linear performance
});
```

**Performance Targets:**
- Entity fetch: <500ms
- Content transformation: <1s
- Total overlay render: <2s
- No regex catastrophic backtracking

#### Scenario 3: Concurrent Sessions

**Setup:**
- 10 concurrent ForgeMode sessions
- Each session generating questions

**Tests:**
```python
# Concurrent session handling
def test_10_concurrent_sessions():
    sessions = []
    start = time.time()

    # Start 10 sessions in parallel
    for i in range(10):
        session = session_service.start_session(
            user_id=f"user_{i}",
            mode="learning",
            topic=f"topic_{i}"
        )
        sessions.append(session)

    elapsed = time.time() - start
    assert elapsed < 5.0  # <5s to start 10 sessions
    assert len(sessions) == 10

    # All sessions can generate questions
    for session in sessions:
        questions = question_service.generate_questions(
            session_id=session.id,
            state="curious"
        )
        assert len(questions) > 0
```

**Performance Targets:**
- 10 concurrent sessions start: <5s
- Each session independently generates questions
- No database locking issues

---

## User Testing Protocols

### "Works for Chris" Validation

**Goal:** System meets creator's daily usage needs

#### Week 1: Baseline Validation
```
Daily Usage Checklist:
- [ ] Capture at least 3 thoughts/day without friction
- [ ] Complete 1 ForgeMode session (any mode)
- [ ] Explore graph for 10+ minutes
- [ ] Resume at least 1 journey from previous day

Success Metrics:
- Capture completion rate >90%
- 0 crashes or data loss events
- Subjective: "System helps me think"

Failure Signals:
- Capture takes >30s (too much friction)
- Questions feel generic (not personalized)
- Can't find past thoughts (retrieval broken)
- Graph exploration feels aimless (no guidance)
```

#### Week 2-4: Extended Validation
```
Weekly Goals:
- [ ] Extract 5+ new concepts from dialogues
- [ ] Build connections between 10+ concepts
- [ ] Identify 2+ thinking patterns from journeys
- [ ] Notice personalization improving

Success Metrics:
- Knowledge graph grows 20+ entities/week
- At least 2 "aha moments" per week
- Questions feel increasingly natural
- System anticipates needs (suggestions accurate)

Failure Signals:
- Graph feels static (no growth)
- Insights not emerging from exploration
- Questions still feel robotic after 20 sessions
- Journey breadcrumbs not useful for reflection
```

### Session Observation Guidelines

**For Dialogue Sessions:**
```
Observe:
1. Question Quality
   - Are questions specific to topic?
   - Do they invite reflection?
   - Do they build on previous answers?

2. Response Flow
   - Does user pause to think before answering?
   - Are responses getting deeper over time?
   - Does user ask for clarification?

3. Insight Emergence
   - Note any "aha" moments
   - Record when user says "I never thought of that"
   - Track connections user makes

Record:
- Session duration
- Number of questions
- Number of responses
- Quality rating (1-5)
- Notable insights
```

**For Flow Explorations:**
```
Observe:
1. Navigation Pattern
   - How does user choose next entity?
   - Does user follow suggested paths?
   - Does user backtrack?

2. Thinking Partner Questions
   - Does user read questions?
   - Does user respond to questions?
   - Do questions guide exploration?

3. Discovery
   - When does user say "interesting"?
   - What connections surprise user?
   - Does user bookmark anything?

Record:
- Exploration duration
- Number of entities visited
- Number of relationships followed
- Breadcrumb depth
- Journey marks created
```

### Feedback Collection Template

**After Each Session:**
```markdown
## Session Feedback

**Date:** YYYY-MM-DD
**Mode:** Capture | Dialogue | Flow
**Duration:** X minutes

### What Worked
- Specific positive experiences
- Features that helped thinking
- Moments of insight

### What Didn't Work
- Friction points
- Confusing UI
- Broken flows

### Questions That Arose
- Unclear functionality
- Expected features not found
- Uncertainty about purpose

### Ideas for Improvement
- Specific suggestions
- Alternative approaches
- Missing features

### Would I Use This Tomorrow?
Yes | Maybe | No — Why?
```

---

## Existing Test Reference

### Backend Tests (94/94 passing)

**Location:** `ies/backend/tests/`

**Test Files:**
| File | Tests | Coverage |
|------|-------|----------|
| `test_book_entities.py` | 6 | Book-entity relationships |
| `test_books_api.py` | 6 | Calibre library API |
| `test_calibre_service.py` | 6 | Calibre metadata queries |
| `test_capture.py` | 4 | Quick Capture flow |
| `test_profile.py` | 6 | User cognitive profiles |
| `test_question_engine.py` | 50 | Question classification, state detection |
| `test_reframe.py` | 3 | Reframe generation |
| `test_session_context.py` | 5 | Session state persistence |
| `test_template.py` | 3 | Thinking templates |
| `test_thinking_and_flow.py` | 5 | Capture → Thinking → Flow pipeline |

**Run Tests:**
```bash
cd ies/backend
uv run pytest                    # All tests
uv run pytest -v                 # Verbose
uv run pytest tests/test_file.py # Specific file
uv run pytest -k "test_name"     # Specific test
```

**Coverage Report:**
```bash
uv run pytest --cov=ies_backend --cov-report=html
# Open htmlcov/index.html
```

### Frontend Tests (Minimal)

**SiYuan Plugin:**
- Build verification: `pnpm build` succeeds
- No unit tests currently
- Manual testing via SiYuan UI

**Readest Integration:**
- Build verification: `pnpm build` succeeds
- TypeScript compilation passes
- No unit tests currently
- Manual testing via Tauri app

**Needed:**
- [ ] Jest/Vitest setup for SiYuan plugin
- [ ] React Testing Library for Readest
- [ ] Component tests for UI interactions
- [ ] Integration tests for cross-component flows

---

## Testing Checklist (Pre-Release)

### Before MVP Release

**Backend:**
- [x] 94/94 tests passing
- [ ] All 3 important TODOs fixed
- [ ] API response times <500ms
- [ ] No memory leaks in long-running processes

**Frontend:**
- [x] SiYuan plugin builds without errors
- [x] Readest builds without errors
- [ ] Manual smoke test: Capture → Dialogue → Flow
- [ ] Manual smoke test: Entity overlay → Flow panel

**Integration:**
- [ ] Cross-app journey resume works
- [ ] Entity extraction from captures works
- [ ] Session documents save with correct metadata
- [ ] Journey breadcrumbs persist across sessions

**User Experience:**
- [ ] Chris uses system for 1 week without major issues
- [ ] At least 20 captures completed
- [ ] At least 5 journeys captured
- [ ] Subjective: "This helps me think"

### Before Alpha Release

**Stability:**
- [ ] No crashes in 2 weeks of daily use
- [ ] No data loss events
- [ ] Auto-ingestion daemon running continuously
- [ ] All Priority 1 bugs fixed

**Performance:**
- [ ] Entity overlay with 200+ entities <2s
- [ ] Graph queries with 1000+ entities <1s
- [ ] Journey synthesis <5s

**Documentation:**
- [ ] User guide complete
- [ ] Troubleshooting section
- [ ] Known issues documented

---

## References

**Test Patterns:**
- `ies/backend/tests/` — Existing test suite
- Pytest documentation
- React Testing Library

**Pressure Testing:**
- `docs/PRESSURE-TEST-PLAN.md` — Systematic evaluation plan
- `docs/CRITICAL-ANALYSIS-2025-12-05.md` — SiYuan pressure test results
- `docs/ANALYSIS-READEST-2025-12-05.md` — Readest pressure test results

**User Testing:**
- `docs/session-notes.md` — Historical session feedback
- `docs/PHASE-1-WORKFLOW.md` — Validated exploration workflow
- `docs/PHASE-2A-VALIDATION-RESULTS.md` — CLI validation results

---

*Testing is not just verification—it's validation that IES delivers on its core promise of thinking partnership.*
